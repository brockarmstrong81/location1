# -*- coding: utf-8 -*-
"""AI Location Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1v9LRROWeFtcnMKdGhe3Gkni5FcH19Jwt
"""

from google.colab import drive
drive.mount('/content/drive')

!apt-get install openjdk-8-jdk-headless -qq > /dev/null

!wget -q https://www-us.apache.org/dist/spark/spark-3.0.1/spark-3.0.1-bin-hadoop2.7.tgz

!pip install -q findspark

import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.0.1-bin-hadoop2.7"

import findspark

!pip install pyspark

from pyspark.sql import SparkSession
import pyspark.sql.functions as F

!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip
!unzip ngrok-stable-linux-amd64.zip
get_ipython().system_raw('./ngrok http 4050 &')
!curl -s http://localhost:4040/api/tunnels

spark = SparkSession.builder.getOrCreate()

import pyspark.sql.functions as F
from pyspark.sql.types import *

dates = ( 
  spark.sql("select month(current_date()-interval 30 day) as cur_month, year(current_date()-interval 30 day) as cur_year, year(current_date()-interval 3 year -interval 30 day) as past_year")
  .withColumn('date1',
                F.date_format(
                  F.expr("make_timestamp(past_year, cur_month, 1, 0, 0, 0)"), 
                  'yyyy-MM-dd')
             )
  .withColumn('date2',
                F.last_day(
                  F.date_format(
                    F.expr("make_timestamp(cur_year, cur_month, 1, 0, 0, 0)"), 
                    'yyyy-MM-dd')
                )
             )
  .withColumn('date3',
                F.date_format(
                  F.expr("make_timestamp(cur_year, cur_month, 1, 0, 0, 0)"), 
                  'yyyy-MM-dd')
             )
  .withColumn('date4',
                F.col('date2')
             )
  .drop(*['cur_month','cur_year', 'past_year'])
  )

date1 = "'"+str(dates.select('date1').collect()[0].date1)+"'"
date2 = "'"+str(dates.select('date2').collect()[0].date2)+"'"
date3 = "'"+str(dates.select('date3').collect()[0].date3)+"'"
date4 = "'"+str(dates.select('date4').collect()[0].date4)+"'"

#date1, date2, date3, date4 = "'2019-09-01'", "'2022-09-30'", "'2022-09-01'", "'2022-09-30'"
print(date1, date2, date3, date4)

parks_canada_postal_latlong = spark.read.load('abfss://databrickadlspocdata@moneris365stdlsdbr01.dfs.core.windows.net/warehouse/datascience.db/postal_latlong')
parks_canada_postal_latlong.createOrReplaceTempView("parks_canada_postal_latlong")

latlongtable = spark.sql("select left(territorypc, 3) as territorypc, avg(TERRITORYPCLATITUDE) as TERRITORYPCLATITUDE, avg(TERRITORYPCLONGITUDE) as TERRITORYPCLONGITUDE from parks_canada_postal_latlong where territorypc is not null and TERRITORYPCLATITUDE is not null and TERRITORYPCLONGITUDE is not null group by left(territorypc, 3)")

latlongtable = latlongtable.withColumn('latlongarray', F.array('territorypc', 'TERRITORYPCLATITUDE', 'TERRITORYPCLONGITUDE'))
mvv_array = [row.latlongarray for row in latlongtable.select('latlongarray').collect()]

df_merchants = spark.sql("select right(mht_itl_nbr, 9) as m_num from mdw_fin_evts_stmt_data.daily_merch_prof where pnt_mht_itl_nbr = 30600059011")

hp_cards = (
  spark.read.table('mdw_fin_evts_stmt_data.sta040_advice_transaction_final')
  .filter("txn_dte between "+date3+" and "+date4)
  .selectExpr('int_card_fingerprint as int_card_fingerprint_b', 'WCSMT_AVC_TXN_TXN_MHT_ITL_NBR AS MERCHANT_NUMBER')
  .join(df_merchants, F.col('merchant_number') == F.col('m_num'))
  .select('int_card_fingerprint_b')
  .distinct()
)

sta040 = (
  spark.read.table('mdw_fin_evts_stmt_data.sta040_advice_transaction_final')
  .filter("txn_dte between "+date1+" and "+date2)
  .filter("wcsmt_avc_txn_txn_typ_cde = 'PURC' and rtrim(WCSMT_AVC_TXN_ECE_MOT_IDN_TXT) NOT IN ('01', '02', '03', '04', '05', '06', '07', '08', '09', '10')")
  .selectExpr('int_card_fingerprint',
             'WCSMT_AVC_TXN_SLT_TXN_TOT_AMT AS amount',
             'WCSMT_AVC_TXN_TXN_MHT_ITL_NBR AS MERCHANT_NUMBER')
  .join(hp_cards, F.col('int_card_fingerprint') == F.col('int_card_fingerprint_b'))
  .drop('int_card_fingerprint_b')
)

daily_merch_prof = spark.sql("select EXT_MCC_CDE, right(mht_itl_nbr, 9) as mht_itl_nbr, left(LCN_ADR_PSL_TXT, 6) as LCN_ADR_PSL_TXT, rtrim(LCN_ADR_SAE_TXT) as LCN_ADR_SAE_TXT from mdw_fin_evts_stmt_data.daily_merch_prof")
postal_latlong = spark.sql("select * from parks_canada_postal_latlong where territorypc is not null and TERRITORYPCLATITUDE is not null and TERRITORYPCLONGITUDE is not null")

sta040 = sta040.join(F.broadcast(daily_merch_prof), F.col('MERCHANT_NUMBER') == F.col('mht_itl_nbr'), "left")
sta040 = sta040.join(F.broadcast(postal_latlong), F.col('LCN_ADR_PSL_TXT') == F.col('territorypc'))

count_df = sta040.selectExpr("int_card_fingerprint", 'territorypclatitude as lat', 'territorypclongitude as long')

count_df = count_df.withColumn('latlong', F.array('lat', 'long'))

bycard = (
  count_df.groupBy('int_card_fingerprint')
  .agg(
    F.collect_list('latlong').alias('latlong'),
    F.count('*').alias('count')
  )
)

from math import sin, cos, sqrt, atan2, radians, degrees
import numpy as np
from sklearn.cluster import DBSCAN

def dbscan(s):
  number_points = len(s)
  coordinate_grouped = []
  identifier_grouped = []
  cluster_centers = []
  if 60 <= number_points < 90:
    dbscan_distance = 2
    min_s_denom = 4
  elif 90 <= number_points < 120:
    dbscan_distance = 2
    min_s_denom = 6
  elif 120 <= number_points < 150:
    dbscan_distance = 2
    min_s_denom = 8
  elif 150 <= number_points <= 1500:
    dbscan_distance = 2
    min_s_denom = 10

  if number_points < 60 or number_points > 1500:
    #print(1)
    output = None    
  else:
    try:
      km_d = dbscan_distance
      min_s = int(number_points/min_s_denom)
      lat_org = []
      long_org = []
      amt_org = [] 
      for i in s:
        lat_org.append(float(i[0]))
        long_org.append(float(i[1]))
      latitude = np.radians(lat_org)
      longitude = np.radians(long_org)
      db_clusters = DBSCAN(eps=km_d/6378, min_samples=min_s, metric='haversine').\
            fit(np.column_stack([longitude,latitude])).labels_.tolist()
      db_clusters = [float(i) for i in db_clusters]
      distinct_value = len(set(db_clusters))

      latlong_pairs = []
      for i in range(len(latitude)):
        latlong_pairs.append([float(lat_org[i]), float(long_org[i])])
      print(db_clusters)
      if -1 in db_clusters:
      #  lower_bound = -1
        distinct_value = distinct_value - 1
      #else:

      lower_bound = 0

      for i in range(lower_bound, distinct_value):
        coordinate_temp = []
        identifier_temp = []
        x = 0
        y = 0
        z = 0
        for j in range(len(db_clusters)):
          if i == db_clusters[j]:
            coordinate_temp.append(latlong_pairs[j])
        coordinate_grouped.append(coordinate_temp)
        identifier_grouped.append(i)

      for i in coordinate_grouped:
          x = 0
          y = 0
          z = 0
          total_amount = 0
          number_coords = len(i)
          mcc_percentage = []

          for j in i:
              x += cos(np.radians(j[0])) * cos(np.radians(j[1]))
              y += cos(np.radians(j[0])) * sin(np.radians(j[1]))
              z += sin(np.radians(j[0]))

          x = x / number_coords
          y = y / number_coords
          z = z / number_coords

          lon = atan2(y, x)
          hyp = sqrt(x*x + y*y)
          lat = atan2(z, hyp)

          latitude = degrees(lat)
          longitude = degrees(lon)

          shortestdistance = sqrt((latitude-float(mvv_array[0][1]))**2+(longitude-float(mvv_array[0][2]))**2)
          closest_psl = mvv_array[0]
          closest_distance = shortestdistance
          for k in mvv_array:
            temp_distance = sqrt((latitude-float(k[1]))**2+(longitude-float(k[2]))**2)
            if temp_distance < closest_distance:
              closest_psl = k
              closest_distance = temp_distance
          cluster_centers.append([[latitude,longitude], closest_psl])

      largest_cluster = 0
      largest_cluster_id = 0

      for i in range(len(coordinate_grouped)):
        cluster_centers[i].append([len(coordinate_grouped[i])])
      cluster_centers.sort(key=lambda s: s[2][0], reverse=True) 

      print('Cluster Centers', cluster_centers)
      print('Coordinate Grouped', coordinate_grouped)
      print('length', len(coordinate_grouped))
      if len(coordinate_grouped) == 0:
        return None
      elif len(coordinate_grouped) == 1:
        return [cluster_centers[0]]
      else:
        return [cluster_centers[:2]]
    except:
      output = None

  return output

dbscan_udf = udf(dbscan, ArrayType(ArrayType(ArrayType(StringType()))))
bycard = bycard
bycard = bycard.select('*', dbscan_udf("latlong").alias("db_scan_centers"))

#bycard.write.format('delta').mode('overwrite').option("overwriteSchema", "true").save('dbfs:/FileStore/tables/poc/datascience/Healthy_Planet/dbscan_'+date1[1:-1]+'_to_'+date2[1:-1])



df = (
  spark.read.table('mdw_fin_evts_stmt_data.sta040_advice_transaction_final')
  .filter("txn_dte between "+date3+" and "+date4)
  .filter("wcsmt_avc_txn_txn_typ_cde in ('PURC', 'RFND')")
  .selectExpr('int_card_fingerprint',
             'WCSMT_AVC_TXN_SLT_TXN_TOT_AMT AS amount',
             'WCSMT_AVC_TXN_TXN_MHT_ITL_NBR AS MERCHANT_NUMBER',
             'txn_dte as TransactionTimestamp',
             'WCSMT_AVC_TXN_ECE_MOT_IDN_TXT',
             'WCSMT_AVC_TXN_POS_ENY_MDE_CDE',
             'wcsmt_avc_txn_txn_typ_cde')
  .join(df_merchants, F.col('merchant_number') == F.col('m_num'))
  .drop('m_num')
)

card_location = (
  spark.read.load('dbfs:/FileStore/tables/poc/datascience/Healthy_Planet/dbscan_'+date1[1:-1]+'_to_'+date2[1:-1])
  .select('int_card_fingerprint', 'db_scan_centers')
  .filter("db_scan_centers is not null")
  .withColumn('origin_fsa', 
              F.when(F.length(F.col('db_scan_centers')[0][1][0])==3,F.col('db_scan_centers')[0][1][0])
              .otherwise(F.col('db_scan_centers')[0][0][1][2:3]))
  .drop(*['db_scan_centers', 'latlong'])
  .withColumnRenamed('int_card_fingerprint', 'fingerprint')
)

merchant_province = spark.sql("select right(mht_itl_nbr, 9) as merch_num, EXT_MCC_CDE, trim(mht_nme_txt) as merchant_name,\
left(LCN_ADR_PSL_TXT, 6) as destination_postal, rtrim(seg_ind) as seg_ind from mdw_fin_evts_stmt_data.daily_merch_prof")

df_data = (
  df
  .join(card_location, F.col('int_card_fingerprint') == F.col('fingerprint'), 'left').drop('fingerprint')
  .join(merchant_province, F.col('MERCHANT_NUMBER') ==  F.col('merch_num'), 'left').drop('merch_num')
  .selectExpr("concat(year(transactiontimestamp), '-', month(TransactionTimestamp)) as FileDate_YM",
              'Origin_FSA',
              'int_card_fingerprint',
              'merchant_number as Destination_merchant_number',
              'merchant_name as Destination_Merchant_Name',
              'destination_postal as Destination_merchant_FSA',
              "case when wcsmt_avc_txn_txn_typ_cde = 'PURC' then amount else 0 end as PURC",
              "case when wcsmt_avc_txn_txn_typ_cde = 'RFND' then amount else 0 end as RFND",
              "case when wcsmt_avc_txn_txn_typ_cde = 'PURC' then 1 else 0 end as txn_count",
              "concat(merchant_name,' ', destination_postal) as Destination_Merchant_Name_fsa"
             )
  .fillna('N/A')
)

df_cardcount = (
  df_data
  .filter('PURC > 0')
  .groupBy('FileDate_YM', 'Origin_FSA', 'Destination_merchant_number', 'Destination_Merchant_Name', 'Destination_merchant_FSA', 'Destination_Merchant_Name_fsa')
  .agg(
    F.countDistinct('int_card_fingerprint').alias('No_of_Cards')
  )
)

df = (
  df_data
  .groupBy('FileDate_YM', 'Origin_FSA', 'Destination_merchant_number','Destination_Merchant_Name', 'Destination_merchant_FSA', 'Destination_Merchant_Name_fsa')
  .agg(
    (F.sum('PURC')-F.sum('RFND')).alias('NetAmt'),
    F.sum('txn_count').alias('TranCount')
  )
  .join(df_cardcount, ['FileDate_YM', 'Origin_FSA', 'Destination_merchant_number', 'Destination_Merchant_Name', 'Destination_merchant_FSA', 'Destination_Merchant_Name_fsa'])
)

canada_post_latlong = (
  spark.read.load('abfss://databrickadlspocdata@moneris365stdlsdbr01.dfs.core.windows.net/warehouse/datascience.db/postal_latlong')
)

latlongtable = (
  canada_post_latlong
  .filter("territorypc is not null and TERRITORYPCLATITUDE is not null and TERRITORYPCLONGITUDE is not null")
  .withColumn('territorypc', F.expr('left(territorypc, 3) as territorypc'))
  .groupBy('territorypc')
  .agg(
    F.avg('TERRITORYPCLATITUDE').alias('TERRITORYPCLATITUDE'),
    F.avg('TERRITORYPCLONGITUDE').alias('TERRITORYPCLONGITUDE')
  )
)

latlongtable_full = (
  canada_post_latlong
  .filter("territorypc is not null and TERRITORYPCLATITUDE is not null and TERRITORYPCLONGITUDE is not null")
  .select('territorypc','TERRITORYPCLATITUDE','TERRITORYPCLONGITUDE')
)

df = (
  df
  .join(latlongtable, F.col('Origin_FSA') == F.col('territorypc'), 'left')
  .drop('territorypc')
  .withColumnRenamed('TERRITORYPCLATITUDE', 'TERRITORYPCLATITUDE_origin')
  .withColumnRenamed('TERRITORYPCLONGITUDE', 'TERRITORYPCLONGITUDE_origin')
  .join(latlongtable_full, F.col('Destination_merchant_FSA') == F.col('territorypc'), 'left')
  .drop('territorypc')
  .withColumnRenamed('TERRITORYPCLATITUDE', 'TERRITORYPCLATITUDE_destination')
  .withColumnRenamed('TERRITORYPCLONGITUDE', 'TERRITORYPCLONGITUDE_destination')
  .withColumn('latlong', F.array(F.col('TERRITORYPCLATITUDE_origin'), F.col('TERRITORYPCLONGITUDE_origin'), F.col('TERRITORYPCLATITUDE_destination'), F.col('TERRITORYPCLONGITUDE_destination')))
)

def distance_calculator(inputs):
  from math import sin, cos, sqrt, atan2, radians
  try:
    # approximate radius of earth in km
    R = 6373.0
    print(inputs)
    lat1 = radians(float(inputs[0]))
    lon1 = radians(float(inputs[1]))
    lat2 = radians(float(inputs[2]))
    lon2 = radians(float(inputs[3]))

    dlon = lon2 - lon1
    dlat = lat2 - lat1

    a = sin(dlat / 2)**2 + cos(lat1) * cos(lat2) * sin(dlon / 2)**2
    c = 2 * atan2(sqrt(a), sqrt(1 - a))

    distance = R * c
  except:
    distance = None
  
  return distance

distance_calculator_udf = udf(distance_calculator, FloatType())

df = (
  df
  .select('*', distance_calculator_udf("latlong").alias("distance"))
  .drop(*['latlong', 'TERRITORYPCLATITUDE_origin', 'TERRITORYPCLONGITUDE_origin', 'TERRITORYPCLATITUDE_destination', 'TERRITORYPCLONGITUDE_destination'])
)

df_g5 = (
  df.filter("Origin_FSA = 'N/A' or no_of_cards < 5")
  .withColumn('Origin_FSA', F.lit('N/A'))
  .groupBy('FileDate_YM', 'Origin_FSA', 'Destination_merchant_number','Destination_Merchant_Name', 'Destination_merchant_FSA', 'Destination_Merchant_Name_fsa')
  .agg(
    F.sum('NetAmt').alias('NetAmt'),
    F.sum('TranCount').alias('TranCount'),
    F.sum('No_of_cards').alias('No_of_cards')
  )
  .withColumn('distance', F.lit('Null'))
)
  
df = (
  df.filter("Origin_FSA != 'N/A' and no_of_cards >= 5")
  .union(df_g5)
)

#df.write.format('delta').mode('overwrite').option("overwriteSchema", "true").save('dbfs:/FileStore/tables/poc/datascience/Healthy_Planet/report_'+date1[1:-1]+'_to_'+date2[1:-1])

import pyspark.sql.functions as F

dates = (
  spark.sql("select month(current_date()-interval 120 day) as cur_month, year(current_date()-interval 120 day) as cur_year, month(current_date()-interval 180 day) as past_month")
  .withColumn('date1',
                F.date_format(
                  F.expr("make_timestamp(cur_year, past_month, 1, 0, 0, 0)"), 
                  'yyyy-MM-dd')
             )
  .withColumn('date2',
                F.last_day(
                  F.date_format(
                    F.expr("make_timestamp(cur_year, cur_month, 1, 0, 0, 0)"), 
                    'yyyy-MM-dd')
                )
             )
  .drop(*['cur_month','cur_year', 'past_year'])
  )

pdate_1 = "'"+str(dates.select('date1').collect()[0].date1)+"'"
pdate_2 = "'"+str(dates.select('date2').collect()[0].date2)+"'"

print(pdate_1, pdate_2)



dates = (
  spark.sql("select month(current_date()-interval 60 day) as cur_month, year(current_date()-interval 60 day) as cur_year, year(current_date()-interval 3 year -interval 60 day) as past_year")
  .withColumn('date1',
                F.date_format(
                  F.expr("make_timestamp(past_year, cur_month, 1, 0, 0, 0)"), 
                  'yyyy-MM-dd')
             )
  .withColumn('date2',
                F.last_day(
                  F.date_format(
                    F.expr("make_timestamp(cur_year, cur_month, 1, 0, 0, 0)"), 
                    'yyyy-MM-dd')
                )
             )
  .withColumn('date3',
                F.date_format(
                  F.expr("make_timestamp(cur_year, cur_month, 1, 0, 0, 0)"), 
                  'yyyy-MM-dd')
             )
  .withColumn('date4',
                F.col('date2')
             )
  .drop(*['cur_month','cur_year', 'past_year'])
  )

pdate1 = "'"+str(dates.select('date1').collect()[0].date1)+"'"
pdate2 = "'"+str(dates.select('date2').collect()[0].date2)+"'"
pdate3 = "'"+str(dates.select('date3').collect()[0].date3)+"'"
pdate4 = "'"+str(dates.select('date4').collect()[0].date4)+"'"

print(pdate1, pdate2, pdate3, pdate4)

df_previous = (
  spark.read.load('dbfs:/FileStore/tables/poc/datascience/Healthy_Planet/report_'+pdate1[1:-1]+'_to_'+pdate2[1:-1])
  .withColumn('Origin_FSA', F.when(F.col('Origin_FSA') == 'N/A', F.lit('N/A')).otherwise(F.lit('withOrigin')))
  .groupBy('Destination_merchant_number', 'Origin_FSA')
  .agg(
    F.sum('netamt').alias(pdate2[1:-1]+'_netamt')
  )
)

df_current = (
  spark.read.load('dbfs:/FileStore/tables/poc/datascience/Healthy_Planet/report_'+date1[1:-1]+'_to_'+date2[1:-1])
  .withColumn('Origin_FSA', F.when(F.col('Origin_FSA') == 'N/A', F.lit('N/A')).otherwise(F.lit('withOrigin')))
  .groupBy('Destination_merchant_number', 'Origin_FSA')
  .agg(
    F.sum('netamt').alias(date2[1:-1]+'_netamt')
  )
)

df_comparison = (
  df_previous
  .join(df_current, ['Destination_merchant_number', 'Origin_FSA'], 'left')
  .withColumn('difference', F.col(date2[1:-1]+'_netamt')/F.col(pdate2[1:-1]+'_netamt'))
)


df_age = spark.read.option("header",True).csv('/FileStore/Census_2021___Age_distribution_by_FSA.csv')
df_age0 = (df_age
                .withColumnRenamed('Age (in single years), average age and median age (128)', 'Age')
                .withColumn('Total_pop', F.col('Gender (3):Total - Gender[1]').cast(DoubleType()))
                .selectExpr(
                     "GEO as FSA",
                     "case when Age in ('Under 1 year', '1','2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14') then Total_pop else 0 end as Child",
                     "case when Age in ('15','16', '17', '18', '19', '20', '21', '22', '23', '24') then Total_pop else 0 end as Youth",
                     "case when Age in ('25','26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38','39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64') then Total_pop else 0 end as Adult",
                     "case when Age in ('65','66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78','79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100 years and over') then Total_pop else 0 end as Older"
                  )
)

df_age1 = (df_age0
  .groupBy('FSA')
  .agg(
      F.sum('Child').alias('Child'),
      F.sum('Youth').alias('Youth'),
      F.sum('Adult').alias('Adult'),
      F.sum('Older').alias('OlderAdult')
      )
 )

from pyspark.sql.functions import udf
commaRep = udf(lambda x: x.replace(',',''))

df_income = spark.read.option("header",True).csv('/FileStore/Income_by_FSA.csv')
df_income1 =(df_income.select('FSA', 'Total Income')
                       .withColumn('TotalAnnualIncome', F.round(commaRep(F.col('Total Income')).cast(DoubleType()), 2)).drop('Total Income')
             )

df = spark.read.load('dbfs:/FileStore/tables/poc/datascience/Healthy_Planet/report_'+date1[1:-1]+'_to_'+date2[1:-1])
df_new = ( df.join(df_age1, F.col('Origin_FSA') == F.col('FSA'), 'left').drop('FSA')
.join(df_income1, F.col('Origin_FSA') == F.col('FSA'), 'left').drop('FSA')
.withColumn('Total_Population', (F.col('Child')+F.col('Youth')+F.col('Adult')+F.col('OlderAdult')))
.withColumn('AverageAnnualIncome', F.round(F.col('TotalAnnualIncome')/F.col('Total_Population'), 2))
.fillna('N/A')
       )

#df_new.write.format('delta').mode('overwrite').option("overwriteSchema", "true").save('dbfs:/FileStore/tables/poc/datascience/Healthy_Planet/report_'+date1[1:-1]+'_to_'+date2[1:-1])